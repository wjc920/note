# 1 创建工程

```shell
mvn archetype:generate \
    -DarchetypeGroupId=org.apache.flink \
    -DarchetypeArtifactId=flink-quickstart-scala \
    -DarchetypeVersion=1.8.0 \
    -DarchetypeCatalog=local \
    -DgroupId=wjc920 \
    -DartifactId=flinkdemo \
    -Dversion=1.0 \
    -Dpackage=wjc920.demo.flinkdemo \
    -DinteractiveMode=false
```

> 注意：  
> 1. `-DarchetypeCatalog=local`不加会导致命令执行时卡住，原因未知。
> 2. 由于`flink-quickstart-scala`没有1.10.1版本，所以可以在生成生成1.8.0版本的项目模板后，修改pom.xml的flink版本。

# 2 WordCount

```scala
import org.apache.flink.streaming.api.scala._
object WordCount {

  def main(args: Array[String]): Unit = {
    //步骤1：指定执行环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    //步骤2：指定数据源
    env.fromElements(WORDS: _*)
    //步骤3：执行转换逻辑
      .map(t=>"Flink Map: "+t)
    //步骤4：指定输出位置
      .addSink(println(_))
    //步骤5：指定任务名称，并触发任务
    env.execute("Flink Streaming Scala API Skeleton")
  }
  val WORDS = Array("Hello Scala", "Hello Java", "Hello Flink")
}
```

以上是Flink流计算任务一般形式，在实际开发过程中会根据需要分别修改五个步骤。

# 3 


## 关于Flink应用开发

flink-dist中包含的Jar
```
org.apache.flink:flink-core
org.apache.flink:flink-java
org.apache.flink:flink-scala_${scala.binary.version}
org.apache.flink:flink-runtime_${scala.binary.version}
org.apache.flink:flink-runtime-web_${scala.binary.version}
org.apache.flink:flink-optimizer_${scala.binary.version}
org.apache.flink:flink-clients_${scala.binary.version}
org.apache.flink:flink-streaming-java_${scala.binary.version}
org.apache.flink:flink-streaming-scala_${scala.binary.version}
org.apache.flink:flink-metrics-core
org.apache.flink:flink-metrics-jmx_${scala.binary.version}
org.apache.flink:flink-mesos_${scala.binary.version}
org.apache.flink:flink-container_${scala.binary.version}
org.apache.flink:flink-statebackend-rocksdb_${scala.binary.version}
org.apache.flink:flink-kubernetes_${scala.binary.version}
org.apache.flink:flink-yarn_${scala.binary.version}


		<!-- Default file system support. The Hadoop and MapR dependencies -->
		<!--       are optional, so not being added to the dist jar        -->
org.apache.flink:flink-hadoop-fs
org.apache.flink:flink-mapr-fs


		<!-- Concrete logging framework - we add this only here (and not in the 
			root POM) to not tie the projects to one specific framework and make
			it easier for users to swap logging frameworks -->

org.slf4j:slf4j-log4j12
log4j:log4j
```

